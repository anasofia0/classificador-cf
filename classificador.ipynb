{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f50f216b-b957-4049-9bf5-804f87af032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/anasofia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "nltk.download('wordnet')\n",
    "#sklearn.decomposition.LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4afaec5-62e0-4ba7-b89c-0356ace2d2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>problem_statement</th>\n",
       "      <th>problem_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>325</td>\n",
       "      <td>A</td>\n",
       "      <td>You are given n rectangles. The corners of rec...</td>\n",
       "      <td>implementation,*1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>B</td>\n",
       "      <td>Daniel is organizing a football tournament. He...</td>\n",
       "      <td>binarysearch,math,*1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>C</td>\n",
       "      <td>Piegirl has found a monster and a book about m...</td>\n",
       "      <td>dfsandsimilar,graphs,shortestpaths,*2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>325</td>\n",
       "      <td>D</td>\n",
       "      <td>In a far away land, there exists a planet shap...</td>\n",
       "      <td>dsu,*2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>325</td>\n",
       "      <td>E</td>\n",
       "      <td>Piegirl found the red button. You have one las...</td>\n",
       "      <td>combinatorics,dfsandsimilar,dsu,graphs,greedy,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contest problem_name                                  problem_statement  \\\n",
       "0      325            A  You are given n rectangles. The corners of rec...   \n",
       "1      325            B  Daniel is organizing a football tournament. He...   \n",
       "2      325            C  Piegirl has found a monster and a book about m...   \n",
       "3      325            D  In a far away land, there exists a planet shap...   \n",
       "4      325            E  Piegirl found the red button. You have one las...   \n",
       "\n",
       "                                        problem_tags  \n",
       "0                               implementation,*1500  \n",
       "1                            binarysearch,math,*1800  \n",
       "2           dfsandsimilar,graphs,shortestpaths,*2600  \n",
       "3                                          dsu,*2900  \n",
       "4  combinatorics,dfsandsimilar,dsu,graphs,greedy,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.fillna('', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae5369c7-a67c-4b55-8e5f-54e8466e5267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantidade de problemas: 8343\n",
      "quantidade de contests: 1424\n",
      "problemas repetidos: 1524\n"
     ]
    }
   ],
   "source": [
    "print(f'quantidade de problemas: {data.shape[0]}')\n",
    "print(f'quantidade de contests: {len(data.contest.unique())}')\n",
    "print(f'problemas repetidos: {data.shape[0] - len(data.problem_statement.unique())}')\n",
    "\n",
    "data.drop_duplicates(subset='problem_statement', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741271ed-5c42-4027-85f1-2fae864fcd0f",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a146feee-1b9c-4316-a09f-6312c8760406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [given, n, rectangles, corners, rectangles, in...\n",
      "1       [Daniel, organizing, football, tournament, com...\n",
      "2       [Piegirl, found, monster, book, monsters, pies...\n",
      "3       [far, away, land, exists, planet, shaped, like...\n",
      "4       [Piegirl, found, red, button, one, last, chanc...\n",
      "                              ...                        \n",
      "8338    [n, blocks, arranged, row, numbered, left, rig...\n",
      "8339    [map, capital, Berland, viewed, infinite, coor...\n",
      "8340    [play, strategic, video, game, yeah, ran, good...\n",
      "8341    [first, let's, define, function, f, x, follows...\n",
      "8342    [Recently, lot, students, enrolled, Berland, S...\n",
      "Name: problem_statement, Length: 6819, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "text = data.problem_statement\n",
    "\n",
    "text = text.apply(lambda x: re.sub('[,|.|(|)|$|!|?|!]',' ',x)) # removendo caracteres especiais\n",
    "text = text.apply(lambda x: [i for i in x.split() if i.lower() not in sw]) # removendo stop-words\n",
    "text = text.apply(lambda x: ' '.join([lemma.lemmatize(i) for i in x])) # lemmatizando todo\n",
    "# bigrama e trigrama todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fdf5159-c4ef-4d94-8790-e0b94aa29119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       given n rectangle corner rectangle integer coo...\n",
       "1       Daniel organizing football tournament come fol...\n",
       "2       Piegirl found monster book monster pie reading...\n",
       "3       far away land exists planet shaped like cylind...\n",
       "4       Piegirl found red button one last chance chang...\n",
       "                              ...                        \n",
       "8338    n block arranged row numbered left right start...\n",
       "8339    map capital Berland viewed infinite coordinate...\n",
       "8340    play strategic video game yeah ran good proble...\n",
       "8341    first let's define function f x follows: \\begi...\n",
       "8342    Recently lot student enrolled Berland State Un...\n",
       "Name: problem_statement, Length: 6819, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabcb1c-9f2c-4c36-a119-ac71fff152b2",
   "metadata": {},
   "source": [
    "## Vertorizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f9d37d5-151d-4cdd-a333-a6ad69c8b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8\n",
    "min_df = 0.05\n",
    "vec = CountVectorizer(max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8822399-0cdf-40a5-9371-2115cd3e768a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6819, 250)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vec.fit_transform(text)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab8d4b-3e5a-49bc-a65b-2b22c00dc4e9",
   "metadata": {},
   "source": [
    "## Agrupando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3ae3aff-ebb0-45af-bc61-577c5e5afdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20 # todo\n",
    "max_iter = 100\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8113b6f-d06b-4bdb-ac25-9aa619420c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_matrix = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "971797f0-150e-4031-ab50-d6c596b3b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['sequence', 'query', 'type', 'answer', 'length', 'one', 'given', 'second', 'following', 'two']\n",
      "Topic 1:  ['problem', 'output', 'use', 'one', 'program', 'task', 'input', 'write', 'given', 'space']\n",
      "Topic 2:  ['time', 'digit', 'position', 'moment', 'show', 'start', 'zero', 'greater', 'sum', '10']\n",
      "Topic 3:  ['day', 'segment', 'friend', 'th', 'one', 'end', 'call', 'exactly', 'next', 'consecutive']\n",
      "Topic 4:  ['point', 'coordinate', 'xi', 'distance', 'two', 'th', 'answer', 'located', 'absolute', 'direction']\n",
      "Topic 5:  ['operation', 'change', 'perform', 'following', 'make', 'minimum', 'equal', 'sequence', 'one', 'initial']\n",
      "Topic 6:  ['leq', '10', 'ldots', 'a_i', 'th', 'example', 'two', 'cdot', 'second', 'next']\n",
      "Topic 7:  ['edge', 'graph', 'path', 'vertex', 'connected', 'two', 'given', 'pair', 'contain', 'one']\n",
      "Topic 8:  ['tree', 'vertex', 'edge', 'two', 'connected', 'given', 'next', 'guaranteed', 'one', 'distance']\n",
      "Topic 9:  ['print', 'without', 'otherwise', 'yes', 'quote', 'two', 'solution', 'part', 'one', 'no']\n",
      "Topic 10:  ['le', '10', 'cdot', 'th', 'example', 'a_i', 'one', 'dots', 'print', 'second']\n",
      "Topic 11:  ['cell', 'row', 'square', 'column', 'left', 'right', 'one', 'th', 'side', 'character']\n",
      "Topic 12:  ['set', 'value', 'sum', 'modulo', 'size', 'two', 'answer', 'given', 'print', 'equal']\n",
      "Topic 13:  ['string', 'letter', 'length', 'character', 'word', 'lowercase', 'consisting', 'example', 'english', 'one']\n",
      "Topic 14:  ['one', 'second', 'th', 'two', 'possible', 'want', 'time', 'get', 'order', 'total']\n",
      "Topic 15:  ['move', 'game', 'play', 'turn', 'one', 'make', 'end', 'second', 'take', 'initially']\n",
      "Topic 16:  ['city', 'pair', 'good', 'two', 'connected', 'one', 'way', 'possible', 'next', 'every']\n",
      "Topic 17:  ['ai', 'space', 'th', 'separated', 'bi', '109', 'print', 'a1', 'a2', '105']\n",
      "Topic 18:  ['array', 'element', 'a_1', 'a_2', 'a_n', 'length', 'second', 'a_i', 'given', 'ldots']\n",
      "Topic 19:  ['test', 'case', 'sum', 'exceed', 'single', 'guaranteed', 'answer', 'one', 'description', 'follows']\n"
     ]
    }
   ],
   "source": [
    "top_word = lda.components_\n",
    "terms = vec.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(top_word):\n",
    "    top_terms_key = zip(terms, comp)\n",
    "    top_terms_key= sorted(top_terms_key, key = lambda t: t[1], reverse=True)[:10]\n",
    "    top_terms_list= list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(i)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6dad-c600-420e-9f7e-f2036eeb93fc",
   "metadata": {},
   "source": [
    "## Validando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01850e29-4b2f-4edf-9cd1-3118669d46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b3410-b983-4193-9f3a-3f3771cbdf70",
   "metadata": {},
   "source": [
    "## Visualizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18f5c0ed-2ecf-454c-a968-ab3915e54a10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m----> 5\u001b[0m panel \u001b[38;5;241m=\u001b[39m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtsne\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m panel\n",
      "File \u001b[0;32m~/Documents/UnB/PIBIC/classificador-cf/env/lib/python3.8/site-packages/pyLDAvis/sklearn.py:94\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(lda_model, dtm, vectorizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create Prepared Data from sklearn's LatentDirichletAllocation and CountVectorizer.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyLDAvis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[0;32m~/Documents/UnB/PIBIC/classificador-cf/env/lib/python3.8/site-packages/pyLDAvis/sklearn.py:38\u001b[0m, in \u001b[0;36m_extract_data\u001b[0;34m(lda_model, dtm, vectorizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_data\u001b[39m(lda_model, dtm, vectorizer):\n\u001b[0;32m---> 38\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[43m_get_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     doc_lengths \u001b[38;5;241m=\u001b[39m _get_doc_lengths(dtm)\n\u001b[1;32m     40\u001b[0m     term_freqs \u001b[38;5;241m=\u001b[39m _get_term_freqs(dtm)\n",
      "File \u001b[0;32m~/Documents/UnB/PIBIC/classificador-cf/env/lib/python3.8/site-packages/pyLDAvis/sklearn.py:20\u001b[0m, in \u001b[0;36m_get_vocab\u001b[0;34m(vectorizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_vocab\u001b[39m(vectorizer):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "panel = pyLDAvis.sklearn.prepare(lda, X, vec, mds='tsne')\n",
    "panel\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c351c6-6309-45a3-ac51-b11ff8b7b4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
